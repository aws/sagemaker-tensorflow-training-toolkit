{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Script Mode to train any TensorFlow script from GitHub in SageMaker\n",
    "\n",
    "In this tutorial, we show how simple it is to train a TensorFlow script in SageMaker using the new Script Mode Tensorflow Container.\n",
    "\n",
    "The example we chose is [Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow](https://github.com/sherjilozair/char-rnn-tensorflow) but this same technique can be used to other scripts or repositories including [TensorFlow Model Zoo](https://github.com/tensorflow/models) and [TensorFlow benchmark scripts](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. It should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role allows SageMaker services to access your data. See the documentation [for how to create these](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sherjilozair/char-rnn-tensorflow > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository includes a README.md with an overview of the project, requirements, and basic usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown('char-rnn-tensorflow/README.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sherlock\n",
    "!wget https://sherlock-holm.es/stories/plain-text/cnus.txt --force-directories --output-document=sherlock/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Mode is in a development phase. We need to construct an Estimator to be able to use it with this example, see [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.estimator import Framework\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "class ScriptModeTensorFlow(Framework):\n",
    "    \"\"\"This class is temporary until the final version of Script Mode is released.\n",
    "    \"\"\"\n",
    "    \n",
    "    __framework_name__ = \"tensorflow-scriptmode-beta\"\n",
    "    \n",
    "    create_model = TensorFlow.create_model\n",
    "    \n",
    "    def __init__(self, py_version='py3', **kwargs):\n",
    "        super(ScriptModeTensorFlow, self).__init__(**kwargs)\n",
    "        self.py_version = py_version\n",
    "        self.image_name = None\n",
    "        self.framework_version = '1.10.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can use [Local Mode](https://github.com/aws/sagemaker-python-sdk#local-mode) to simulate SageMaker locally before submit training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hyperparameters = {'num_epochs': 1, \n",
    "                   'data_dir': '/opt/ml/input/data/training',\n",
    "                   'save_dir': '/opt/ml/model'}\n",
    "\n",
    "estimator = ScriptModeTensorFlow(entry_point='train.py',\n",
    "                                 source_dir='char-rnn-tensorflow',\n",
    "                                 train_instance_type='local', \n",
    "                                 train_instance_count=1,\n",
    "                                 hyperparameters=hyperparameters,\n",
    "                                 role=role)\n",
    "\n",
    "estimator.fit({'training': 'file://%s' % os.path.join(os.getcwd(), 'sherlock')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Script Mode executes the script in the container\n",
    "\n",
    "The cell above downloads a Python 3 CPU container locally and simulates SageMaker training. When training starts, script mode installs the user script as a Python module. The module name matches the script name. In the case above, **train.py** is transformed into a Python module named **train**.\n",
    "\n",
    "After that, the Python interpreter executes the user module, passing **hyperparameters** as script arguments. The example above will be executed as follow:\n",
    "```bash\n",
    "python -m train --num-epochs 1 --data-dir /opt/ml/input/data/training --save-dir /opt/ml/model\n",
    "```\n",
    "\n",
    "A user provide script consumes the hyperparameters using any argument parsing library, [in the example above](https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/train.py#L11):\n",
    "\n",
    "```python\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare', help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='save', help='directory to store checkpointed models')\n",
    "...\n",
    "args = parser.parse_args()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Let's explain the values of **--data_dir** and **--save-dir**:\n",
    "\n",
    "- **/opt/ml/input/data/training** is the directory inside the container where the training data is downloaded. The data was downloaded in this folder because **training** is the channel name defined in ```estimator.fit({'training': inputs})```. See [training data](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-trainingdata) for more information. \n",
    "\n",
    "- **/opt/ml/model** use this directory to save models, checkpoints or any other data. Any data saved in this folder is saved in the S3 bucket defined for training. See [model data](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-envvariables) for more information.\n",
    "\n",
    "### Reading additional information from the container\n",
    "\n",
    "Very often, a user script needs additional information from the container that is not available in ```hyperparameters```.\n",
    "SageMaker Containers writes this information as **environment variables** that are available inside the script.\n",
    "\n",
    "For example, the example above can read information about the **training** channel provided in the training job request:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  # reads input channels training and testing from the environment variables\n",
    "  parser.add_argument('--data_dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "```\n",
    "\n",
    "Script Mode displays the list of the environment variables available in the training logs. You can find the [entire list here](https://github.com/aws/sagemaker-containers/blob/master/README.md#environment-variables-full-specification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to upload the dataset to an S3 bucket so SageMaker can access the data during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='sherlock', bucket=bucket, key_prefix='datasets/sherlock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the estimator argument **train_instance_type** to any SageMaker ml instance available for training. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ScriptModeTensorFlow(entry_point='train.py',\n",
    "                                source_dir='char-rnn-tensorflow',\n",
    "                                train_instance_type='ml.c4.xlarge', \n",
    "                                train_instance_count=1,\n",
    "                                hyperparameters=hyperparameters,\n",
    "                                role=role)\n",
    "\n",
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing additional requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing pip packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script Mode will install your source_dir in the container as a [Python package](https://github.com/aws/sagemaker-containers/blob/master/src/sagemaker_containers/_modules.py#L100). You can include a [requirements.txt file in the root folder of your source_dir to install any pip dependencies](https://github.com/aws/sagemaker-containers/blob/master/src/sagemaker_containers/_modules.py#L111). You can, for example, install the lastest version of tensorflow in the container:\n",
    "\n",
    "content of requirements.txt\n",
    "```\n",
    "tensorflow==1.11.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing apt-get packages and other dependencies\n",
    "You can define a setup.py file in your source_dir to install other dependencies. The example below will install [TensorFlow for C](https://www.tensorflow.org/install/lang_c) in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tf_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_c/get-tf-c.sh\n",
    "\n",
    "wget -q -t 3 https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.11.0.tar.gz\n",
    "tar -xzvf libtensorflow-cpu-linux-x86_64-1.11.0.tar.gz -C /usr/local\n",
    "\n",
    "ldconfig\n",
    "\n",
    "gcc -I/usr/local/include -L/usr/local/lib hello_tf.c -ltensorflow -o hello_tf\n",
    "cp hello_tf /usr/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_c/hello_tf.c\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <tensorflow/c/c_api.h>\n",
    "\n",
    "int main() {\n",
    "  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_c/setup.py\n",
    "from distutils.command.build_py import build_py as _build_py\n",
    "from distutils.core import setup\n",
    "import subprocess\n",
    "\n",
    "class build_py(_build_py):\n",
    "    def run(self):\n",
    "        subprocess.check_output(['bash', './get-tf-c.sh'])\n",
    "\n",
    "        super(build_py, self).run()\n",
    "\n",
    "\n",
    "from setuptools import setup\n",
    "setup(packages=[''],\n",
    "      name=\"test\",\n",
    "      version='1.0.0',\n",
    "      cmdclass={'build_py': build_py},\n",
    "      include_package_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tf_c/train_c.py\n",
    "\n",
    "import subprocess\n",
    "\n",
    "message = subprocess.check_output('hello_tf')\n",
    "assert message == b'Hello from TensorFlow C library version 1.11.0\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ScriptModeTensorFlow(entry_point='train_c.py',\n",
    "                                 source_dir='tf_c',\n",
    "                                 train_instance_type='local', \n",
    "                                 train_instance_count=1,\n",
    "                                 role=role)\n",
    "\n",
    "estimator.fit({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
